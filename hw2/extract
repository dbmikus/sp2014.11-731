#!/usr/bin/env python

from collections import defaultdict
import argparse
import json
import os, sys, math
import lcs
import subprocess

def debug_print(string):
  string = str(string)
  sys.stderr.write(string + '\n')

def extract_features(vf, hyp, ref):
  hwords = hyp.lower().split()
  rwords = ref.lower().split()
  refset = set(rwords)

  meteor = extract_simple_meteor(hwords, rwords)
  # vector_score = extract_vector_score(vf, hwords, rwords)
  return {'meteor': meteor}


# Determines the number of fewest possible number of chunks shared between
# hwords and rwords.
# A chunk is a sequence of adjacent unigrams.
# We do this by repeatedly finding the largest shared chunk until there are no
# more chunks.
def get_num_chunks(hwords, rwords):
  # the number of chunks
  num_chunks = 0
  # the number of unigrams across all chunks
  num_grams = 0

  # We continually find and remove longest common substrings until there are
  # none left.
  # For each one that we remove, we increase the number of chunks we've counted.
  # We only remove from the hwords sequence. This is because each unigram in the
  # hypothesis can map to at most one unigram in the reference. Thus, once we
  # have mapped a chunk from the hypothesis, that chunk can no longer map.
  # However, a chunk in the reference can cover two separate chunks in the
  # hypothesis.
  (seq, start, end) = lcs.LCS(hwords, rwords)
  while seq != []:
    num_chunks += 1
    num_grams += len(seq)
    hwords = hwords[:start] + hwords[end:]
    (seq, start, end) = lcs.LCS(hwords, rwords)
  return (num_chunks, num_grams)


def extract_simple_meteor(hwords, rwords):
  trunc_length = 5
  full_match_weight = 0.8
  stem_match_weight = 0.9

  # The weight of the recall
  # Precision is (1 - alpha)
  alpha = 0.9
  beta = 5.0
  gamma = 0.25

  # We truncate every word down to the first six characters
  hwords_trunc = [ word[:trunc_length] for word in hwords ]
  rwords_trunc = [ word[:trunc_length] for word in rwords ]
  refset = set(rwords)
  refset_trunc = set(rwords_trunc)

  # m is the number of unigrams in the candidate translation that are also found
  # in the reference translation
  m = sum(full_match_weight for word in hwords if word in refset)
  m_t = sum(stem_match_weight for word in hwords_trunc if word in refset_trunc)

  # Precision
  P = float(m + m_t) / float(len(hwords) * full_match_weight
                             + len(hwords_trunc) * stem_match_weight)
  # Recall
  R = float(m + m_t) / float(len(rwords) * full_match_weight
                             + len(rwords_trunc) * stem_match_weight)

  F_mean_den = (alpha * P + (1-alpha) * R)
  if (F_mean_den != 0.0):
    F_mean = (P * R) / F_mean_den
  else:
    F_mean = 0.0

  # Calculating the penalty score based on shared chunks
  (num_chunks, num_grams) = get_num_chunks(hwords, rwords)
  (num_chunks_t, num_grams_t) = get_num_chunks(hwords_trunc, rwords_trunc)
  nc = num_chunks * full_match_weight + num_chunks_t * stem_match_weight
  ng = num_grams * full_match_weight + num_grams_t * stem_match_weight
  # I don't know why we subtract one, but that's what it says to do in the
  # slides
  # chunk_penalty = float(num_chunks - 1) / float(num_grams - 1)
  if (ng - 1) == 0:
    chunk_penalty = 1.0
  else:
    chunk_penalty = float(nc - 1) / float(ng - 1)

  # Calculate the discounting factor
  DF = gamma * (chunk_penalty**beta)
  score = F_mean * (1 - DF)

  return score


# We compute a score based on word vectors
def extract_vector_score(vf, hwords, rwords):
  total_score = 0.0
  # For each word in the reference, we get the vector distance to similar words.
  # We will then check the words in the hypothesis to see if they match any of
  # the words that share a vector distance with the reference word and compute a
  # score value based on that.
  for rword in rwords:
    # We get the vector distances for the current reference word
    dist_proc = subprocess.Popen(['distance', vf],
                                 stdin=subprocess.PIPE,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
    vec_res = dist_proc.communicate(input=rword + '\nEXIT\n')[0].split('\n')[1:]
    vec_res[0] = vec_res[0].split()

    # Checks if the word was found in our vector distance search
    if vec_res[0][-1] != '-1':
      # These are the actual word distance values
      dist_vals = vec_res[4:-1]
      # Go through all of the potential vector matches and split them up into
      # a dictionary with key = word, and value = distance
      dist_map = defaultdict(float)
      for dist_val in dist_vals:
        word_and_dist = dist_val.split()
        dist_map[word_and_dist[0]] = float(word_and_dist[1])
      total_score += vector_match_score(hwords, dist_map)

  return total_score


def vector_match_score(hwords, dist_map):
  score = 0.0
  for word in hwords:
    score += dist_map[word]

  return score



argparser = argparse.ArgumentParser(prog='extract')
argparser.add_argument('-x', '--pairs', dest='pairs', default='data/en-cs.pairs',
                       help='Reference-Hypothesis pairs')
argparser.add_argument('--vector-file', dest='vector_file', required=True,
                       help='The file that contains word vectors')

args = argparser.parse_args()

lc = 0
sys.stderr.write('Extracting features for (ref,hyp) pairs from %s' % args.pairs)
# loop over all (ref,hyp) pairs in the input file and extract evaluation features
for ref_hyp in open(args.pairs):
  if (lc % 500) == 0:
    sys.stderr.write('.')
  lc += 1
  ref, hyp = ref_hyp.rstrip().split(' ||| ')
  fmap = extract_features(args.vector_file, hyp, ref)
  print json.dumps(fmap)   # print evaluation feature map
