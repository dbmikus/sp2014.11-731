#!/usr/bin/env python

import argparse
import json
import os, sys, math

def debug_print(string):
  sys.stderr.write(string + '\n')

def extract_features(hyp, ref):
  hwords = hyp.lower().split()
  rwords = ref.lower().split()
  refset = set(rwords)

  meteor = extract_simple_meteor(hwords, rwords)
  return {'meteor': meteor}

def extract_simple_meteor(hwords, rwords):
  # The weight of the recall
  # Precision is (1 - alpha)
  alpha = 0.9

  # We truncate every word down to the first six characters
  hwords_trunc = [ word[:6] for word in hwords ]
  rwords_trunc = [ word[:6] for word in rwords ]
  refset = set(rwords)
  refset_trunc = set(rwords_trunc)

  # m is the number of unigrams in the candidate translation that are also found
  # in the reference translation
  m = sum(1 for word in hwords if word in refset)
  m_t = sum(1 for word in hwords_trunc if word in refset_trunc)

  # Precision
  P = float(m + m_t) / float(len(hwords) + len(hwords_trunc))
  # Recall
  R = float(m + m_t) / float(len(rwords) + len(rwords_trunc))

  F_mean_den = (alpha * P + (1-alpha) * R)
  if (F_mean_den != 0.0):
    F_mean = (P * R) / F_mean_den
  else:
    F_mean = 0.0

  return F_mean




argparser = argparse.ArgumentParser(prog='extract')
argparser.add_argument('-x', '--pairs', dest='pairs', default='data/en-cs.pairs', help='Reference-Hypothesis pairs')

args = argparser.parse_args()

lc = 0
sys.stderr.write('Extracting features for (ref,hyp) pairs from %s.\n' % args.pairs)
# loop over all (ref,hyp) pairs in the input file and extract evaluation features
for ref_hyp in open(args.pairs):
  lc += 1
  ref, hyp = ref_hyp.rstrip().split(' ||| ')
  fmap = extract_features(hyp, ref)
  print json.dumps(fmap)   # print evaluation feature map
