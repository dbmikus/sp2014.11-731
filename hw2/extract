#!/usr/bin/env python

import argparse
import json
import os, sys, math
import lcs

def debug_print(string):
  string = str(string)
  sys.stderr.write(string + '\n')

def extract_features(hyp, ref):
  hwords = hyp.lower().split()
  rwords = ref.lower().split()
  refset = set(rwords)

  meteor = extract_simple_meteor(hwords, rwords)
  return {'meteor': meteor}


# Determines the number of fewest possible number of chunks shared between
# hwords and rwords.
# A chunk is a sequence of adjacent unigrams.
# We do this by repeatedly finding the largest shared chunk until there are no
# more chunks.
def get_num_chunks(hwords, rwords):
  # the number of chunks
  num_chunks = 0
  # the number of unigrams across all chunks
  num_grams = 0

  # We continually find and remove longest common substrings until there are
  # none left.
  # For each one that we remove, we increase the number of chunks we've counted.
  # We only remove from the hwords sequence. This is because each unigram in the
  # hypothesis can map to at most one unigram in the reference. Thus, once we
  # have mapped a chunk from the hypothesis, that chunk can no longer map.
  # However, a chunk in the reference can cover two separate chunks in the
  # hypothesis.
  (seq, start, end) = lcs.LCS(hwords, rwords)
  while seq != []:
    num_chunks += 1
    num_grams += len(seq)
    hwords = hwords[:start] + hwords[end:]
    (seq, start, end) = lcs.LCS(hwords, rwords)
  return (num_chunks, num_grams)


def extract_simple_meteor(hwords, rwords):
  trunc_length = 5
  full_match_weight = 0.8
  stem_match_weight = 0.9

  # The weight of the recall
  # Precision is (1 - alpha)
  alpha = 0.9
  beta = 5.0
  gamma = 0.25

  # We truncate every word down to the first six characters
  hwords_trunc = [ word[:trunc_length] for word in hwords ]
  rwords_trunc = [ word[:trunc_length] for word in rwords ]
  refset = set(rwords)
  refset_trunc = set(rwords_trunc)

  # m is the number of unigrams in the candidate translation that are also found
  # in the reference translation
  m = sum(full_match_weight for word in hwords if word in refset)
  m_t = sum(stem_match_weight for word in hwords_trunc if word in refset_trunc)

  # Precision
  P = float(m + m_t) / float(len(hwords) * full_match_weight
                             + len(hwords_trunc) * stem_match_weight)
  # Recall
  R = float(m + m_t) / float(len(rwords) * full_match_weight
                             + len(rwords_trunc) * stem_match_weight)

  F_mean_den = (alpha * P + (1-alpha) * R)
  if (F_mean_den != 0.0):
    F_mean = (P * R) / F_mean_den
  else:
    F_mean = 0.0

  # Calculating the penalty score based on shared chunks
  (num_chunks, num_grams) = get_num_chunks(hwords, rwords)
  (num_chunks_t, num_grams_t) = get_num_chunks(hwords_trunc, rwords_trunc)
  nc = num_chunks * full_match_weight + num_chunks_t * stem_match_weight
  ng = num_grams * full_match_weight + num_grams_t * stem_match_weight
  # I don't know why we subtract one, but that's what it says to do in the
  # slides
  # chunk_penalty = float(num_chunks - 1) / float(num_grams - 1)
  if (ng - 1) == 0:
    chunk_penalty = 1.0
  else:
    chunk_penalty = float(nc - 1) / float(ng - 1)

  # Calculate the discounting factor
  DF = gamma * (chunk_penalty**beta)
  score = F_mean * (1 - DF)

  return score




argparser = argparse.ArgumentParser(prog='extract')
argparser.add_argument('-x', '--pairs', dest='pairs', default='data/en-cs.pairs', help='Reference-Hypothesis pairs')

args = argparser.parse_args()

lc = 0
sys.stderr.write('Extracting features for (ref,hyp) pairs from %s.\n' % args.pairs)
# loop over all (ref,hyp) pairs in the input file and extract evaluation features
for ref_hyp in open(args.pairs):
  lc += 1
  ref, hyp = ref_hyp.rstrip().split(' ||| ')
  fmap = extract_features(hyp, ref)
  print json.dumps(fmap)   # print evaluation feature map
