#!/usr/bin/env python
import argparse
import sys
import models
import heapq
from collections import namedtuple


def main():
    parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
    parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
    parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
    parser.add_argument('-s', '--stack-size', dest='s', default=1, type=int, help='Maximum stack size (default=1)')
    parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
    parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
    opts = parser.parse_args()

    tm = models.TM(opts.tm, sys.maxint)
    lm = models.LM(opts.lm)
    sys.stderr.write('Decoding %s...\n' % (opts.input,))
    input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

    # A hypothesis tuple object.
    #   logprob: the current probability of the hypothesis in log space
    #   logfuture: the estimated future cost of the rest of the translation in
    #              log space
    #   lm_state: the language model state, necessary to interact with LM class
    #   trans_array: a boolean array indicating which words have been translated
    #   predecessor: the previous hypothesis we had, which is used to
    #                reconstruct phrase sequence as a string
    #   src_ps: the index of the starting word of the source phrase
    #   src_pe: the index of the ending word of the source phrase
    #   phrase: the target phrase that we are adding in this hypothesis
    hypothesis = namedtuple('hypothesis',
                            'logprob, logfuture, lm_state, trans_array, predecessor, src_ps, src_pe, phrase')
    for f in input_sents:
        print "\n"
        trans_array = []
        for i in xrange(0, len(f)):
            trans_array.append(False)

        # The following code implements a DP monotone decoding
        # algorithm (one that doesn't permute the target phrases).
        # Hence all hypotheses in stacks[i] represent translations of
        # the first i words of the input sentence.
        # HINT: Generalize this so that stacks[i] contains translations
        # of any i words (remember to keep track of which words those
        # are, and to estimate future costs)
        initial_hypothesis = hypothesis(0.0, 0.0, lm.begin(),
                                        None, None,
                                        None, None, None)

        stacks = [{} for _ in f] + [{}]
        stacks[0][lm.begin()] = initial_hypothesis
        # iterate over every stack except for the last one, since the last stack has
        # the fully decoded sentences, and thus we cannot do any more work on it
        for i, stack in enumerate(stacks[:-1]):
            # extend the top s hypotheses in the current stack
            for h in heapq.nlargest(opts.s, stack.itervalues(), key=lambda h: h.logprob): # prune
                prev_start = h.src_ps
                prev_end = h.src_pe
                src_phrases = source_phrase_options(f, tm, trans_array, prev_start, prev_end)

                for (s, e, src_phrase) in src_phrases:
                    if src_phrase in tm:
                        for tgt_phrase in tm[src_phrase]:
                            # TODO how do we estimate future cost?
                            logprob = h.logprob + tgt_phrase.logprob
                            lm_state = h.lm_state
                            for word in tgt_phrase.english.split():
                                (lm_state, word_logprob) = lm.score(lm_state, word)
                                logprob += word_logprob
                            nt = num_trans(trans_array)
                            logprob += lm.end(lm_state) if nt == len(trans_array) else 0.0
                            new_trans = update_trans(trans_array, s, e)
                            new_hypothesis = hypothesis(logprob, h.logfuture, lm_state,
                                                        new_trans, h, s, e, tgt_phrase)
                            if lm_state not in stacks[nt] or stacks[nt][lm_state].logprob < logprob: # second case is recombination
                                stacks[nt][lm_state] = new_hypothesis

        # find best translation by looking at the best scoring hypothesis
        # on the last stack
        winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
        def extract_english_recursive(h):
            return '' if h.predecessor is None else '%s%s ' % (extract_english_recursive(h.predecessor), h.phrase.english)
        print extract_english_recursive(winner)

        if opts.verbose:
            def extract_tm_logprob(h):
                return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
            tm_logprob = extract_tm_logprob(winner)
            sys.stderr.write('LM = %f, TM = %f, Total = %f\n' %
                (winner.logprob - tm_logprob, tm_logprob, winner.logprob))


def update_trans(trans_array, start, end):
    trans_array = list(trans_array)
    for i in xrange(start, end+1):
        if (trans_array[i]):
            sys.stderr.write("Tried to translate something already translated\n")
        trans_array[i] = True
    return trans_array

# Counts the number of translated words in the translation array
def num_trans(trans_array):
    i = 0
    for word_trans in trans_array:
        if word_trans:
            i += 1

    return i


# f: the foreign source sentence we are trying to find phrases in
# tm: the translation model that contains possible translation phrases from
#     source phrase
# trans_array: which words in the source have been translated already
# prev_start: the start index of the previous phrase that we translated
# prev_end: the end index of the previous phrase that we translated.
#
# If prev_start and prev_end are None (because we don't yet have a previous
# hypothesis), then we just make sure that the distance from the start of the
# sentence is not too great.
def source_phrase_options(f, tm, trans_array, prev_start, prev_end):
    dist_limit = 5

    if prev_start is None:
        # The earliest index that our phrase can start at
        i = 0
        # The latest index that our phrase can start at
        j = dist_limit - 1
    else:
        # The earliest index that our phrase can start at
        i = max(0, prev_end + 1 - dist_limit)
        # The latest index that our phrase can start at
        j = min(prev_end + 1 + dist_limit, len(f)-1)

    vrs = valid_ranges(trans_array, i, j)

    # A list of tuples, where each element is a possible phrase.
    # Each tuple has: (phrase_start, phrase_end, phrase)
    phrase_opts = []

    for (s, e) in vrs:
        phrase_opts.append((s, e, f[s:e+1]))
    return phrase_opts



# Gets all valid ranges of phrases, restricting the start of the phrase to not
# go past the start and end indices, and also to not including any previously
# translated words. start and end are inclusive indices.
def valid_ranges(trans_array, start, end):
    valid_ranges = []

    vr_start = None
    vr_end = None
    for i in xrange(start, end+1):
        # the i'th word is not yet translated
        if not trans_array[i]:
            for j in xrange(i, len(trans_array)):
                if not trans_array[j]:
                    valid_ranges.append((i, j))
                # If we hit an already translated word, it cannot be part of a
                # phrase, so we must break out from finding any longer phrase
                # end points given our current start index
                else:
                    break
        # We don't need any else statement here. If the i'th index is already
        # translated, we simply don't do anything and skip to the next part

    return valid_ranges


if (__name__ == "__main__"):
    main()
